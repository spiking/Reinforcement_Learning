{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Deep_Q_Agent():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear')) # LEFT OR RIGHT\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform action from given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(model, state, epsilon):\n",
    "    action = -1\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = random.randrange(3)\n",
    "    else:\n",
    "        action = agent.predict(state.reshape(-1, 2))\n",
    "        action = np.argmax(action)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline - Pre-train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size, epsilon, done):\n",
    "    \n",
    "    # Get a minibatch of previous training memories\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    #print(\"Replay: Train model on sample of previous memories\")\n",
    "\n",
    "    # For each memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        \n",
    "        if not done:\n",
    "            # Predict future disconted reward if not a terminating step\n",
    "            target = (reward + gamma * np.amax(agent.predict(next_state)))\n",
    "        else:\n",
    "            # Terminating step, no prediction needed\n",
    "            target = reward\n",
    "        \n",
    "        # Map an approximation between current state to the future discounted reward\n",
    "        target_f = agent.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        # Train model to find map function from state to target\n",
    "        # This will maximize the future reward based on current state\n",
    "        agent.fit(state, target_f, epochs=1, verbose=0)\n",
    "    \n",
    "    # Decrease exploration successively\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, name):\n",
    "    return model.load_weights(name)\n",
    "\n",
    "def save(model, name):\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n # left, right\n",
    "memory = deque(maxlen=1000)\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1  # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.005\n",
    "episodes = 1000\n",
    "batch_size = 10\n",
    "max_score = -1.2\n",
    "max_score_ep = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming_score_threshold = -195\n",
    "nbr_of_games = 1000\n",
    "nbr_of_steps_per_game = 200\n",
    "        \n",
    "env.reset()\n",
    "\n",
    "def train_agent():\n",
    "    \n",
    "    game_memory = []\n",
    "    scores = []\n",
    "    \n",
    "    for game in range(nbr_of_games):\n",
    "        \n",
    "        env.reset()\n",
    "        game_score = 0\n",
    "        memory_current_game = []\n",
    "        state = []\n",
    "        next_state = [] \n",
    "        \n",
    "        for step in range(nbr_of_steps_per_game):\n",
    "            \n",
    "            action = random.randrange(0, 3)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(state) > 0:\n",
    "                memory_current_game.append((state, action))\n",
    "\n",
    "            if next_state[0] > -0.25:\n",
    "                reward = 0.5\n",
    "                \n",
    "            next_state = np.reshape(next_state, [1, state_size])    \n",
    "            state = next_state\n",
    "            game_score += reward\n",
    "            \n",
    "            if done: \n",
    "                #print(game_score)\n",
    "                break\n",
    " \n",
    "        if game_score >= gaming_score_threshold:\n",
    "        \n",
    "            scores.append(game_score)\n",
    "            for action in memory_current_game:\n",
    "                if action[1] == 0:\n",
    "                    target = [1, 0, 0]\n",
    "                elif action[1] == 1:\n",
    "                    target = [0, 1, 0]\n",
    "                elif action[1] == 2:\n",
    "                    target = [0, 0, 1]\n",
    "                game_memory.append((action[0], target))\n",
    "                #print(\"Added\")\n",
    "        \n",
    "    return scores, game_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, game_memory = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6169/6169 [==============================] - 1s 170us/step - loss: 0.2266 - acc: 0.3385\n",
      "Epoch 2/5\n",
      "6169/6169 [==============================] - 1s 99us/step - loss: 0.2231 - acc: 0.3359\n",
      "Epoch 3/5\n",
      "6169/6169 [==============================] - 1s 99us/step - loss: 0.2223 - acc: 0.3524\n",
      "Epoch 4/5\n",
      "6169/6169 [==============================] - 1s 97us/step - loss: 0.2219 - acc: 0.3550\n",
      "Epoch 5/5\n",
      "6169/6169 [==============================] - 1s 98us/step - loss: 0.2217 - acc: 0.3501\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_130 (Dense)            (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,371\n",
      "Trainable params: 2,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "X = np.array([m[0] for m in game_memory]).reshape(-1, 2)\n",
    "y = np.array([m[1] for m in game_memory]).reshape(-1, 3)\n",
    "agent = build_Deep_Q_Agent()\n",
    "agent.fit(X, y, epochs=5)\n",
    "agent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "results = []\n",
    "epsilon = 0.25\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    max_score = 0\n",
    "    score = 0\n",
    "\n",
    "    for trial in range(200):\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done and trial < 199: \n",
    "            print(\"Victory!\")\n",
    "            break\n",
    "        if done:\n",
    "            print(\"Loss!\")\n",
    "            break\n",
    "    \n",
    "    env.reset()\n",
    "    results.append(score)\n",
    "\n",
    "    # p: [[-0.4041538  0.       ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n # left, right\n",
    "memory = deque(maxlen=20000)\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1  # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.005\n",
    "episodes = 1000\n",
    "batch_size = 10\n",
    "max_score = -1.2\n",
    "max_score_ep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next episode!\n",
      "\n",
      "Episode: 0/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.40631064368071\n",
      "Size:  0\n",
      "Next episode!\n",
      "\n",
      "Episode: 1/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.27534386631173\n",
      "Size:  0\n",
      "Next episode!\n",
      "\n",
      "Episode: 2/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.52279607015083\n",
      "Size:  0\n",
      "Next episode!\n",
      "\n",
      "Episode: 3/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.77352532701339\n",
      "Size:  0\n",
      "Next episode!\n",
      "\n",
      "Episode: 4/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-106.79388662807949\n",
      "Size:  0\n",
      "Next episode!\n",
      "\n",
      "Episode: 5/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.53419483139369\n",
      "Size:  0\n",
      "Next episode!\n",
      "\n",
      "Episode: 6/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-98.06599309851799\n",
      "Size:  200\n",
      "Next episode!\n",
      "\n",
      "Episode: 7/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.03778637819957\n",
      "Size:  200\n",
      "Next episode!\n",
      "\n",
      "Episode: 8/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.95045782921733\n",
      "Size:  200\n",
      "Next episode!\n",
      "\n",
      "Episode: 9/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-101.75803102806587\n",
      "Size:  400\n",
      "Next episode!\n",
      "\n",
      "Episode: 10/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-103.6197393572533\n",
      "Size:  400\n",
      "Next episode!\n",
      "\n",
      "Episode: 11/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.58523521642272\n",
      "Size:  400\n",
      "Next episode!\n",
      "\n",
      "Episode: 12/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-107.18770494101476\n",
      "Size:  400\n",
      "Next episode!\n",
      "\n",
      "Episode: 13/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.2928566617673\n",
      "Size:  400\n",
      "Next episode!\n",
      "\n",
      "Episode: 14/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-108.16765008801801\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 15/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-106.4415393275235\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 16/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.20980637624744\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 17/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.92439512584939\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 18/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-106.44604723734885\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 19/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.85756743205205\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 20/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-106.08799889712161\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 21/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-103.82276582906363\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 22/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.59516924251805\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 23/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-107.03762892501369\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 24/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-103.49746749368893\n",
      "Size:  600\n",
      "Next episode!\n",
      "\n",
      "Episode: 25/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-101.042058121104\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 26/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-107.69074709272223\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 27/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-106.82214120608072\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 28/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-106.19565725626803\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 29/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.42791197422979\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 30/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.47847206961765\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 31/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-104.99773566250404\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 32/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-103.899287497389\n",
      "Size:  800\n",
      "Next episode!\n",
      "\n",
      "Episode: 33/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-101.76429952599283\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 34/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-103.73275172694822\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 35/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-103.96148555996754\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 36/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-107.40296588704032\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 37/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.06993618820981\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 38/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.82731446572117\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 39/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-105.10955387713611\n",
      "Size:  1000\n",
      "Next episode!\n",
      "\n",
      "Episode: 40/1000, Score: 0, Epsilon: 1.0, Top score: 0\n",
      "Trial:  199\n",
      "-108.20711588222782\n",
      "Size:  1200\n",
      "Next episode!\n",
      "\n",
      "Episode: 41/1000, Score: 0, Epsilon: 0.369, Top score: 0\n",
      "Trial:  199\n",
      "-105.13984442562702\n",
      "Size:  1200\n",
      "Next episode!\n",
      "\n",
      "Episode: 42/1000, Score: 0, Epsilon: 0.136, Top score: 0\n",
      "Trial:  199\n",
      "-90.3685945842612\n",
      "Size:  1400\n",
      "Next episode!\n",
      "\n",
      "Episode: 43/1000, Score: 0, Epsilon: 0.0502, Top score: 0\n",
      "Trial:  199\n",
      "-98.32110650146257\n",
      "Size:  1600\n",
      "Next episode!\n",
      "\n",
      "Episode: 44/1000, Score: 0, Epsilon: 0.0185, Top score: 0\n",
      "Trial:  199\n",
      "-96.01218127926472\n",
      "Size:  1800\n",
      "Next episode!\n",
      "\n",
      "Episode: 45/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-78.93246713032917\n",
      "Size:  2000\n",
      "Next episode!\n",
      "\n",
      "Episode: 46/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-85.01114543461557\n",
      "Size:  2200\n",
      "Next episode!\n",
      "\n",
      "Episode: 47/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-86.21567859486153\n",
      "Size:  2400\n",
      "Next episode!\n",
      "\n",
      "Episode: 48/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-83.53586207535979\n",
      "Size:  2600\n",
      "Next episode!\n",
      "\n",
      "Episode: 49/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-80.90546667884513\n",
      "Size:  2800\n",
      "Next episode!\n",
      "\n",
      "Episode: 50/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-95.8637376585947\n",
      "Size:  3000\n",
      "Next episode!\n",
      "\n",
      "Episode: 51/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-94.98389254498096\n",
      "Size:  3200\n",
      "Next episode!\n",
      "\n",
      "Episode: 52/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-59.12397926890603\n",
      "Size:  3400\n",
      "Next episode!\n",
      "\n",
      "Episode: 53/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-89.66362510277138\n",
      "Size:  3600\n",
      "Next episode!\n",
      "\n",
      "Episode: 54/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-91.02856078768207\n",
      "Size:  3800\n",
      "Next episode!\n",
      "\n",
      "Episode: 55/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-92.38972330416017\n",
      "Size:  4000\n",
      "Next episode!\n",
      "\n",
      "Episode: 56/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-91.25251034722388\n",
      "Size:  4200\n",
      "Next episode!\n",
      "\n",
      "Episode: 57/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-82.45021225844071\n",
      "Size:  4400\n",
      "Next episode!\n",
      "\n",
      "Episode: 58/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-97.98029034856562\n",
      "Size:  4600\n",
      "Next episode!\n",
      "\n",
      "Episode: 59/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-95.42859799040085\n",
      "Size:  4800\n",
      "Next episode!\n",
      "\n",
      "Episode: 60/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-80.89649687759393\n",
      "Size:  5000\n",
      "Next episode!\n",
      "\n",
      "Episode: 61/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-68.50552449778807\n",
      "Size:  5200\n",
      "Next episode!\n",
      "\n",
      "Episode: 62/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-76.35578293647308\n",
      "Size:  5400\n",
      "Next episode!\n",
      "\n",
      "Episode: 63/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-86.63154785361579\n",
      "Size:  5600\n",
      "Next episode!\n",
      "\n",
      "Episode: 64/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-93.5930397794542\n",
      "Size:  5800\n",
      "Next episode!\n",
      "\n",
      "Episode: 65/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-82.55706454649093\n",
      "Size:  6000\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 66/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  185\n",
      "-82.89977013943509\n",
      "Size:  6186\n",
      "Next episode!\n",
      "\n",
      "Episode: 67/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-70.19841295721257\n",
      "Size:  6386\n",
      "Next episode!\n",
      "\n",
      "Episode: 68/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-94.3410172042441\n",
      "Size:  6586\n",
      "Next episode!\n",
      "\n",
      "Episode: 69/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-100.66193866927999\n",
      "Size:  6786\n",
      "Next episode!\n",
      "\n",
      "Episode: 70/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-95.0712081343397\n",
      "Size:  6986\n",
      "Next episode!\n",
      "\n",
      "Episode: 71/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-91.4094973280874\n",
      "Size:  7186\n",
      "Next episode!\n",
      "\n",
      "Episode: 72/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-85.965643562585\n",
      "Size:  7386\n",
      "Next episode!\n",
      "\n",
      "Episode: 73/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-82.88942684428517\n",
      "Size:  7586\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 74/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  154\n",
      "-66.68877437532562\n",
      "Size:  7741\n",
      "Next episode!\n",
      "\n",
      "Episode: 75/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-70.9362894986351\n",
      "Size:  7941\n",
      "Next episode!\n",
      "\n",
      "Episode: 76/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-62.12741580556692\n",
      "Size:  8141\n",
      "Next episode!\n",
      "\n",
      "Episode: 77/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-66.57853123987621\n",
      "Size:  8341\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 78/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  171\n",
      "-81.52252418806512\n",
      "Size:  8513\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 79/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  151\n",
      "-40.01691325307657\n",
      "Size:  8665\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 80/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  108\n",
      "-48.390102452823506\n",
      "Size:  8774\n",
      "Next episode!\n",
      "\n",
      "Episode: 81/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-78.47868251116034\n",
      "Size:  8974\n",
      "Next episode!\n",
      "\n",
      "Episode: 82/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-67.9307204805556\n",
      "Size:  9174\n",
      "Next episode!\n",
      "\n",
      "Episode: 83/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-80.19406167646505\n",
      "Size:  9374\n",
      "Next episode!\n",
      "\n",
      "Episode: 84/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-86.01370187120456\n",
      "Size:  9574\n",
      "Next episode!\n",
      "\n",
      "Episode: 85/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-79.99991324530606\n",
      "Size:  9774\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 86/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  157\n",
      "-71.07250214272139\n",
      "Size:  9932\n",
      "Next episode!\n",
      "\n",
      "Episode: 87/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-89.31899782696571\n",
      "Size:  10132\n",
      "Next episode!\n",
      "\n",
      "Episode: 88/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-70.65219634643881\n",
      "Size:  10332\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 89/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  176\n",
      "-66.05342006740845\n",
      "Size:  10509\n",
      "Next episode!\n",
      "\n",
      "Episode: 90/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-78.22900166628736\n",
      "Size:  10709\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 91/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  129\n",
      "-51.34416209490595\n",
      "Size:  10839\n",
      "Next episode!\n",
      "\n",
      "Episode: 92/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-84.2306951591159\n",
      "Size:  11039\n",
      "Next episode!\n",
      "\n",
      "Episode: 93/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-60.0943894541942\n",
      "Size:  11239\n",
      "Next episode!\n",
      "\n",
      "Episode: 94/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-103.22610592822011\n",
      "Size:  11239\n",
      "Next episode!\n",
      "\n",
      "Episode: 95/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-85.9406735611855\n",
      "Size:  11439\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 96/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  158\n",
      "-66.33344965293118\n",
      "Size:  11598\n",
      "Next episode!\n",
      "\n",
      "Episode: 97/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-97.92565109885213\n",
      "Size:  11798\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 98/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  189\n",
      "-77.37658070983892\n",
      "Size:  11988\n",
      "Next episode!\n",
      "\n",
      "Episode: 99/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-83.04473700766711\n",
      "Size:  12188\n",
      "Next episode!\n",
      "\n",
      "Episode: 100/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-80.94335348835428\n",
      "Size:  12388\n",
      "Next episode!\n",
      "\n",
      "Episode: 101/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-92.94568553406206\n",
      "Size:  12588\n",
      "Next episode!\n",
      "\n",
      "Episode: 102/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-56.482255341897925\n",
      "Size:  12788\n",
      "Next episode!\n",
      "Victory!\n",
      "\n",
      "Episode: 103/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  194\n",
      "-65.09282885525644\n",
      "Size:  12983\n",
      "Next episode!\n",
      "\n",
      "Episode: 104/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-107.68085729815695\n",
      "Size:  12983\n",
      "Next episode!\n",
      "\n",
      "Episode: 105/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-84.18589051866869\n",
      "Size:  13183\n",
      "Next episode!\n",
      "\n",
      "Episode: 106/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-57.02380727467587\n",
      "Size:  13383\n",
      "Next episode!\n",
      "\n",
      "Episode: 107/1000, Score: 0, Epsilon: 0.00999, Top score: 0\n",
      "Trial:  199\n",
      "-77.31648120852353\n",
      "Size:  13583\n",
      "Next episode!\n"
     ]
    }
   ],
   "source": [
    "agent = build_Deep_Q_Agent()\n",
    "#agent = load(agent, 'weights/cartpole-dqn.h5')\n",
    "#epsilon = 0.01\n",
    "done = False\n",
    "results = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    max_score = 0\n",
    "    score = 0\n",
    "    ep_score = 0\n",
    "    ep_mem = []\n",
    "\n",
    "    for trial in range(200):\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        state_pos = next_state[0]   \n",
    "        \n",
    "        if state_pos > -0.25: # -1 far left, 0 far right\n",
    "            reward = 0.5\n",
    "            \n",
    "        ep_score += state_pos\n",
    "            \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        ep_mem.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        if done and trial < 199: \n",
    "            print(\"Victory!\")\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            print(\"\\nEpisode: {}/{}, Score: {}, Epsilon: {:.3}, Top score: {}\".format(episode, episodes, score, float(epsilon), max_score))\n",
    "            print(\"Trial: \", trial)\n",
    "            \n",
    "            if ep_score > -103 or ep_score < -108: # IF NO RENDERING, IF RENDER [103, 108]\n",
    "                [remember(state, action, reward, next_state, done) for state, action, reward, next_state, done in ep_mem]\n",
    "\n",
    "            print(ep_score)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Sample random minibatches from memory and train model to \n",
    "        # maximize future reward from current state\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        if len(memory) >= 1000:\n",
    "            epsilon = replay(batch_size, epsilon, done)\n",
    "            \n",
    "    print(\"Size: \", len(memory))\n",
    "\n",
    "    #if episode % 50 == 0:\n",
    "    #    print(\"Save weights!\")\n",
    "    #    save(agent, \"weights/mountain-car-dqn-\" + str(episode) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
