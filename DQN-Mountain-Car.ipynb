{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "**STATE**\n",
    "\n",
    "**ACTIONS**\n",
    "\n",
    "**REWARD**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Deep_Q_Agent():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear')) # LEFT OR RIGHT\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate), metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform action from given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(model, state, epsilon):\n",
    "    action = -1\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = random.randrange(3)\n",
    "    else:\n",
    "        action = agent.predict(state.reshape(-1, 2))\n",
    "        action = np.argmax(action)\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent to maximize future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size, epsilon, done):\n",
    "    \n",
    "    # Get a minibatch of previous training memories\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    #print(\"Replay: Train model on sample of previous memories\")\n",
    "\n",
    "    # For each memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        \n",
    "        if not done:\n",
    "            # Predict future disconted reward if not a terminating step\n",
    "            target = (reward + gamma * np.amax(agent.predict(next_state)[0]))\n",
    "        else:\n",
    "            # Terminating step, no prediction needed\n",
    "            target = reward\n",
    "        \n",
    "        # Map an approximation between current state to the future discounted reward\n",
    "        target_f = agent.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        # Train model to find map function from state to target\n",
    "        # This will maximize the future reward based on current state\n",
    "        agent.fit(state, target_f, epochs=1, verbose=0)\n",
    "    \n",
    "    # Decrease exploration successively\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, name):\n",
    "    return model.load_weights(name)\n",
    "\n",
    "def save(model, name):\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n # left, right\n",
    "memory = deque(maxlen=1000)\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1  # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.005\n",
    "episodes = 1000\n",
    "batch_size = 10\n",
    "max_score = -1.2\n",
    "max_score_ep = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming_score_threshold = -195\n",
    "nbr_of_games = 10000\n",
    "nbr_of_steps_per_game = 200\n",
    "        \n",
    "env.reset()\n",
    "\n",
    "def train_agent():\n",
    "    \n",
    "    game_memory = []\n",
    "    scores = []\n",
    "    \n",
    "    for game in range(nbr_of_games):\n",
    "        \n",
    "        env.reset()\n",
    "        game_score = 0\n",
    "        memory_current_game = []\n",
    "        state = []\n",
    "        next_state = [] \n",
    "        \n",
    "        for step in range(nbr_of_steps_per_game):\n",
    "            \n",
    "            action = random.randrange(0, 3)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(state) > 0:\n",
    "                memory_current_game.append((state, action))\n",
    "\n",
    "            if next_state[0] > -0.25:\n",
    "                reward = 0.5\n",
    "                \n",
    "            next_state = np.reshape(next_state, [1, state_size])    \n",
    "            state = next_state\n",
    "            game_score += reward\n",
    "            \n",
    "            if done: \n",
    "                #print(game_score)\n",
    "                break\n",
    " \n",
    "        if game_score >= gaming_score_threshold:\n",
    "        \n",
    "            scores.append(game_score)\n",
    "            for action in memory_current_game:\n",
    "                if action[1] == 0:\n",
    "                    target = [1, 0, 0]\n",
    "                elif action[1] == 1:\n",
    "                    target = [0, 1, 0]\n",
    "                elif action[1] == 2:\n",
    "                    target = [0, 0, 1]\n",
    "                game_memory.append((action[0], target))\n",
    "                print(\"Added\")\n",
    "        \n",
    "    return scores, game_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, game_memory = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(game_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([m[0] for m in game_memory]).reshape(-1, 2)\n",
    "y = np.array([m[1] for m in game_memory]).reshape(-1, 3)\n",
    "agent = build_Deep_Q_Agent()\n",
    "agent.fit(X, y, epochs=5)\n",
    "agent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "results = []\n",
    "epsilon = 0.25\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    max_score = 0\n",
    "    score = 0\n",
    "\n",
    "    for trial in range(200):\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done and trial < 199: \n",
    "            print(\"Victory!\")\n",
    "            break\n",
    "        if done:\n",
    "            print(\"Loss!\")\n",
    "            break\n",
    "    \n",
    "    env.reset()\n",
    "    results.append(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n # left, right\n",
    "memory = deque(maxlen=20000)\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1  # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.005\n",
    "episodes = 1000\n",
    "batch_size = 100\n",
    "max_score = -1.2\n",
    "max_score_ep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = build_Deep_Q_Agent()\n",
    "#agent = load(agent, 'weights/cartpole-dqn.h5')\n",
    "epsilon = 0.01\n",
    "done = False\n",
    "results = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    max_score = 0\n",
    "    score = 0\n",
    "    print(\"Next episode!\")\n",
    "\n",
    "    for trial in range(200):\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        state_pos = next_state[0]\n",
    "        \n",
    "        if state_pos > -0.25:\n",
    "            reward = 0.5     \n",
    "            \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if state_pos > -0.4:\n",
    "            remember(state, action, reward, next_state, done) \n",
    "        #state = next_state\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            print(\"\\nEpisode: {}/{}, Score: {}, Epsilon: {:.3}, Top score: {}\".format(episode, episodes, score, float(epsilon), max_score))\n",
    "            print(\"Trial: \", trial)\n",
    "            #results.append(max_score)\n",
    "            \n",
    "            #if episode % 1 == 0:\n",
    "            #    print(\"Average score: \", sum(results[-25:]) / 25.0) \n",
    "            \n",
    "            break\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Sample random minibatches from memory and train model to \n",
    "        # maximize future reward from current state\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        if len(memory) > 5000:\n",
    "            epsilon = replay(batch_size, epsilon, done)\n",
    "            \n",
    "    print(\"Size: \", len(memory))\n",
    "\n",
    "    #if episode % 50 == 0:\n",
    "    #    print(\"Save weights!\")\n",
    "    #    save(agent, \"weights/mountain-car-dqn-\" + str(episode) + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
