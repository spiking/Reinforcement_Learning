{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "**STATE**\n",
    "\n",
    "* Cart Position\n",
    "* Cart Velocity\n",
    "* Pole Angle\n",
    "* Pole Velocity at tip\n",
    "\n",
    "**ACTIONS**\n",
    "* 0 - LEFT\n",
    "* 1 - RIGHT\n",
    "\n",
    "**REWARD**\n",
    "\n",
    "* 1 for every survived step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Deep_Q_Agent():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(8, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear')) # LEFT OR RIGHT\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform action from given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(model, state, epsilon):\n",
    "    action = -1\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        #print(\"Perform random action!\")\n",
    "        action = random.randrange(2)\n",
    "    else:\n",
    "        #print(\"Perform predicted action from current state!\")\n",
    "        # Predict the most likely reward value from the given state\n",
    "        action = model.predict(state)\n",
    "        # Perform the action based on the predicted reward\n",
    "        action = np.argmax(action[0]) # [0.74, 0.2] LEFT / RIGHT\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    #print(\"Remember state, action performed, reward received, etc\")\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent to maximize future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size, epsilon, done):\n",
    "    \n",
    "    # Get a minibatch of previous training memories\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    #print(\"Replay: Train model on sample of previous memories\")\n",
    "\n",
    "    # For each memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        \n",
    "        if not done:\n",
    "            # Predict future disconted reward if not a terminating step\n",
    "            target = (reward + gamma * np.amax(agent.predict(next_state)[0]))\n",
    "        else:\n",
    "            # Terminating step, no prediction needed\n",
    "            target = reward\n",
    "        \n",
    "        # Map an approximation between current state to the future discounted reward\n",
    "        target_f = agent.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        # Train model to find map function from state to target\n",
    "        # This will maximize the future reward based on current state\n",
    "        agent.fit(state, target_f, epochs=1, verbose=0)\n",
    "    \n",
    "    # Decrease exploration successively\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, name):\n",
    "    return model.load_weights(name)\n",
    "\n",
    "def save(model, name):\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n # left, right\n",
    "memory = deque(maxlen=10000)\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 0.5  # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.0005\n",
    "episodes = 1000\n",
    "batch_size = 25\n",
    "max_score = 0\n",
    "max_score_ep = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(range(len(results)), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 0/1000, Score: 34, Epsilon: 0.422, Top score: 0\n",
      "Average score:  1.36\n",
      "Save weights!\n",
      "\n",
      "Episode: 1/1000, Score: 19, Epsilon: 0.383, Top score: 34\n",
      "\n",
      "Episode: 2/1000, Score: 9, Epsilon: 0.366, Top score: 34\n",
      "\n",
      "Episode: 3/1000, Score: 10, Epsilon: 0.349, Top score: 34\n",
      "\n",
      "Episode: 4/1000, Score: 14, Epsilon: 0.325, Top score: 34\n",
      "\n",
      "Episode: 5/1000, Score: 9, Epsilon: 0.311, Top score: 34\n",
      "\n",
      "Episode: 6/1000, Score: 10, Epsilon: 0.295, Top score: 34\n",
      "\n",
      "Episode: 7/1000, Score: 10, Epsilon: 0.281, Top score: 34\n",
      "\n",
      "Episode: 8/1000, Score: 9, Epsilon: 0.269, Top score: 34\n",
      "\n",
      "Episode: 9/1000, Score: 11, Epsilon: 0.254, Top score: 34\n",
      "\n",
      "Episode: 10/1000, Score: 16, Epsilon: 0.235, Top score: 34\n",
      "\n",
      "Episode: 11/1000, Score: 13, Epsilon: 0.22, Top score: 34\n",
      "\n",
      "Episode: 12/1000, Score: 8, Epsilon: 0.211, Top score: 34\n",
      "\n",
      "Episode: 13/1000, Score: 14, Epsilon: 0.197, Top score: 34\n",
      "\n",
      "Episode: 14/1000, Score: 9, Epsilon: 0.188, Top score: 34\n",
      "\n",
      "Episode: 15/1000, Score: 8, Epsilon: 0.181, Top score: 34\n",
      "\n",
      "Episode: 16/1000, Score: 9, Epsilon: 0.173, Top score: 34\n",
      "\n",
      "Episode: 17/1000, Score: 9, Epsilon: 0.165, Top score: 34\n",
      "\n",
      "Episode: 18/1000, Score: 9, Epsilon: 0.158, Top score: 34\n",
      "\n",
      "Episode: 19/1000, Score: 9, Epsilon: 0.151, Top score: 34\n",
      "\n",
      "Episode: 20/1000, Score: 13, Epsilon: 0.141, Top score: 34\n",
      "\n",
      "Episode: 21/1000, Score: 12, Epsilon: 0.133, Top score: 34\n",
      "\n",
      "Episode: 22/1000, Score: 27, Epsilon: 0.116, Top score: 34\n",
      "\n",
      "Episode: 23/1000, Score: 27, Epsilon: 0.102, Top score: 34\n",
      "\n",
      "Episode: 24/1000, Score: 13, Epsilon: 0.0952, Top score: 34\n",
      "\n",
      "Episode: 25/1000, Score: 21, Epsilon: 0.0856, Top score: 34\n",
      "Average score:  12.72\n",
      "\n",
      "Episode: 26/1000, Score: 17, Epsilon: 0.0786, Top score: 34\n",
      "\n",
      "Episode: 27/1000, Score: 18, Epsilon: 0.0719, Top score: 34\n",
      "\n",
      "Episode: 28/1000, Score: 31, Epsilon: 0.0615, Top score: 34\n",
      "\n",
      "Episode: 29/1000, Score: 37, Epsilon: 0.0511, Top score: 34\n",
      "\n",
      "Episode: 30/1000, Score: 101, Epsilon: 0.0308, Top score: 37\n",
      "\n",
      "Episode: 31/1000, Score: 73, Epsilon: 0.0214, Top score: 101\n",
      "\n",
      "Episode: 32/1000, Score: 55, Epsilon: 0.0162, Top score: 101\n",
      "\n",
      "Episode: 33/1000, Score: 69, Epsilon: 0.0115, Top score: 101\n",
      "\n",
      "Episode: 34/1000, Score: 40, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 35/1000, Score: 36, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 36/1000, Score: 41, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 37/1000, Score: 47, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 38/1000, Score: 58, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 39/1000, Score: 45, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 40/1000, Score: 50, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 41/1000, Score: 32, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 42/1000, Score: 42, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 43/1000, Score: 27, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 44/1000, Score: 28, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 45/1000, Score: 15, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 46/1000, Score: 30, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 47/1000, Score: 30, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 48/1000, Score: 51, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 49/1000, Score: 109, Epsilon: 0.00997, Top score: 101\n",
      "\n",
      "Episode: 50/1000, Score: 80, Epsilon: 0.00997, Top score: 109\n",
      "Average score:  46.48\n",
      "Save weights!\n",
      "\n",
      "Episode: 51/1000, Score: 34, Epsilon: 0.00997, Top score: 109\n",
      "\n",
      "Episode: 52/1000, Score: 57, Epsilon: 0.00997, Top score: 109\n",
      "\n",
      "Episode: 53/1000, Score: 43, Epsilon: 0.00997, Top score: 109\n",
      "\n",
      "Episode: 54/1000, Score: 160, Epsilon: 0.00997, Top score: 109\n",
      "\n",
      "Episode: 55/1000, Score: 66, Epsilon: 0.00997, Top score: 160\n",
      "\n",
      "Episode: 56/1000, Score: 66, Epsilon: 0.00997, Top score: 160\n",
      "\n",
      "Episode: 57/1000, Score: 192, Epsilon: 0.00997, Top score: 160\n",
      "\n",
      "Episode: 58/1000, Score: 52, Epsilon: 0.00997, Top score: 192\n",
      "\n",
      "Episode: 59/1000, Score: 88, Epsilon: 0.00997, Top score: 192\n",
      "\n",
      "Episode: 60/1000, Score: 218, Epsilon: 0.00997, Top score: 192\n",
      "\n",
      "Episode: 61/1000, Score: 116, Epsilon: 0.00997, Top score: 218\n",
      "\n",
      "Episode: 62/1000, Score: 316, Epsilon: 0.00997, Top score: 218\n",
      "\n",
      "Episode: 63/1000, Score: 204, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 64/1000, Score: 208, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 65/1000, Score: 278, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 66/1000, Score: 226, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 67/1000, Score: 197, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 68/1000, Score: 231, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 69/1000, Score: 288, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 70/1000, Score: 214, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 71/1000, Score: 220, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 72/1000, Score: 207, Epsilon: 0.00997, Top score: 316\n",
      "\n",
      "Episode: 73/1000, Score: 185, Epsilon: 0.00997, Top score: 316\n"
     ]
    }
   ],
   "source": [
    "agent = build_Deep_Q_Agent()\n",
    "#agent = load(agent, 'weights/cartpole-dqn.h5')\n",
    "#epsilon = 0.01\n",
    "done = False\n",
    "results = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for frame in range(episodes):\n",
    "        env.render()\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # Select action - combine prediction and randomness\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done: reward = -1\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Move to next state and save transition in memory\n",
    "        # ---------------------------------------------------------\n",
    "            \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done) \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(\"\\nEpisode: {}/{}, Score: {}, Epsilon: {:.3}, Top score: {}\".format(episode, episodes, frame, float(epsilon), max_score))\n",
    "            \n",
    "            results.append(frame)\n",
    "            \n",
    "            if episode % 25 == 0:\n",
    "                print(\"Average score: \", sum(results[-25:]) / 25.0)\n",
    "\n",
    "            if frame > max_score:\n",
    "                max_score = frame\n",
    "                max_score_ep = episode\n",
    "            \n",
    "            break\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Sample random minibatches from memory and train model to \n",
    "        # maximize future reward from current state\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            epsilon = replay(batch_size, epsilon, done)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(\"Save weights!\")\n",
    "        save(agent, \"weights/cartpole-dqn-\" + str(episode) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
