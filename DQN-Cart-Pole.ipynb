{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "**STATE**\n",
    "\n",
    "* Cart Position\n",
    "* Cart Velocity\n",
    "* Pole Angle\n",
    "* Pole Velocity at tip\n",
    "\n",
    "**ACTIONS**\n",
    "* 0 - LEFT\n",
    "* 1 - RIGHT\n",
    "\n",
    "**REWARD**\n",
    "\n",
    "* 1 for every survived step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Deep_Q_Agent():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(8, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear')) # LEFT OR RIGHT\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform action from given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(agent, state, epsilon):\n",
    "    action = -1\n",
    "    \n",
    "    # action[0] = [P_LEFT, P_RIGHT]\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = random.randrange(2)\n",
    "    else:\n",
    "        # maximize Q(s,a) (state-action function)\n",
    "        action = agent.predict(state)\n",
    "        action = np.argmax(action[0]) \n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent to maximize future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size, epsilon, done):\n",
    "    \n",
    "    # Get a minibatch of previous training memories\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    # For each memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        \n",
    "        if not done:\n",
    "            # Predict from next_state the maximal future discounted reward\n",
    "            n_state_pred_reward = (reward + gamma * np.amax(agent.predict(next_state)[0]))\n",
    "        else:\n",
    "            n_state_pred_reward = reward\n",
    "        \n",
    "        \n",
    "        # Predict from current_state the maximal future discounted reward depeding on action taken\n",
    "        c_state_pred_reward = agent.predict(state)\n",
    "        c_state_pred_reward[0][action] = n_state_pred_reward\n",
    "        \n",
    "        \n",
    "        # Train agent to map current state to future discounted reward\n",
    "        agent.fit(state, current_state_pred_reward, epochs=1, verbose=0)\n",
    "    \n",
    "    # Decrease exploration successively\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, name):\n",
    "    return model.load_weights(name)\n",
    "\n",
    "def save(model, name):\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "memory = deque(maxlen=10000) # D-memory (transitions)\n",
    "gamma = 0.95 # discount rate\n",
    "epsilon = 0.5 # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.0005\n",
    "episodes = 1000\n",
    "batch_size = 25\n",
    "max_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(range(len(results)), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average score:  1.48\n",
      "\n",
      "Episode: 0/1000, Score: 37, Epsilon: 0.471, Top score: 37\n",
      "\n",
      "Episode: 1/1000, Score: 11, Epsilon: 0.446, Top score: 37\n",
      "\n",
      "Episode: 2/1000, Score: 10, Epsilon: 0.424, Top score: 37\n",
      "\n",
      "Episode: 3/1000, Score: 9, Epsilon: 0.405, Top score: 37\n",
      "\n",
      "Episode: 4/1000, Score: 7, Epsilon: 0.391, Top score: 37\n",
      "\n",
      "Episode: 5/1000, Score: 13, Epsilon: 0.366, Top score: 37\n",
      "\n",
      "Episode: 6/1000, Score: 11, Epsilon: 0.347, Top score: 37\n",
      "\n",
      "Episode: 7/1000, Score: 17, Epsilon: 0.318, Top score: 37\n",
      "\n",
      "Episode: 8/1000, Score: 9, Epsilon: 0.304, Top score: 37\n",
      "\n",
      "Episode: 9/1000, Score: 11, Epsilon: 0.288, Top score: 37\n",
      "\n",
      "Episode: 10/1000, Score: 16, Epsilon: 0.266, Top score: 37\n",
      "\n",
      "Episode: 11/1000, Score: 8, Epsilon: 0.255, Top score: 37\n",
      "\n",
      "Episode: 12/1000, Score: 10, Epsilon: 0.243, Top score: 37\n",
      "\n",
      "Episode: 13/1000, Score: 10, Epsilon: 0.231, Top score: 37\n",
      "\n",
      "Episode: 14/1000, Score: 7, Epsilon: 0.223, Top score: 37\n",
      "\n",
      "Episode: 15/1000, Score: 10, Epsilon: 0.212, Top score: 37\n",
      "\n",
      "Episode: 16/1000, Score: 10, Epsilon: 0.202, Top score: 37\n",
      "\n",
      "Episode: 17/1000, Score: 12, Epsilon: 0.19, Top score: 37\n",
      "\n",
      "Episode: 18/1000, Score: 9, Epsilon: 0.182, Top score: 37\n",
      "\n",
      "Episode: 19/1000, Score: 9, Epsilon: 0.174, Top score: 37\n",
      "\n",
      "Episode: 20/1000, Score: 10, Epsilon: 0.165, Top score: 37\n",
      "\n",
      "Episode: 21/1000, Score: 8, Epsilon: 0.159, Top score: 37\n",
      "\n",
      "Episode: 22/1000, Score: 8, Epsilon: 0.152, Top score: 37\n",
      "\n",
      "Episode: 23/1000, Score: 7, Epsilon: 0.147, Top score: 37\n",
      "\n",
      "Episode: 24/1000, Score: 9, Epsilon: 0.141, Top score: 37\n",
      "\n",
      "Average score:  10.04\n",
      "\n",
      "Episode: 25/1000, Score: 10, Epsilon: 0.134, Top score: 37\n",
      "\n",
      "Episode: 26/1000, Score: 22, Epsilon: 0.12, Top score: 37\n",
      "\n",
      "Episode: 27/1000, Score: 15, Epsilon: 0.111, Top score: 37\n",
      "\n",
      "Episode: 28/1000, Score: 15, Epsilon: 0.103, Top score: 37\n",
      "\n",
      "Episode: 29/1000, Score: 18, Epsilon: 0.0942, Top score: 37\n",
      "\n",
      "Episode: 30/1000, Score: 14, Epsilon: 0.0878, Top score: 37\n",
      "\n",
      "Episode: 31/1000, Score: 20, Epsilon: 0.0794, Top score: 37\n",
      "\n",
      "Episode: 32/1000, Score: 88, Epsilon: 0.0511, Top score: 88\n",
      "\n",
      "Episode: 33/1000, Score: 98, Epsilon: 0.0313, Top score: 98\n",
      "\n",
      "Episode: 34/1000, Score: 45, Epsilon: 0.025, Top score: 98\n",
      "\n",
      "Episode: 35/1000, Score: 31, Epsilon: 0.0214, Top score: 98\n",
      "\n",
      "Episode: 36/1000, Score: 33, Epsilon: 0.0181, Top score: 98\n",
      "\n",
      "Episode: 37/1000, Score: 20, Epsilon: 0.0164, Top score: 98\n",
      "\n",
      "Episode: 38/1000, Score: 16, Epsilon: 0.0151, Top score: 98\n",
      "\n",
      "Episode: 39/1000, Score: 20, Epsilon: 0.0137, Top score: 98\n",
      "\n",
      "Episode: 40/1000, Score: 21, Epsilon: 0.0123, Top score: 98\n",
      "\n",
      "Episode: 41/1000, Score: 24, Epsilon: 0.0109, Top score: 98\n",
      "\n",
      "Episode: 42/1000, Score: 33, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 43/1000, Score: 27, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 44/1000, Score: 22, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 45/1000, Score: 21, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 46/1000, Score: 22, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 47/1000, Score: 25, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 48/1000, Score: 29, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 49/1000, Score: 31, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Average score:  29.0\n",
      "\n",
      "Episode: 50/1000, Score: 15, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 51/1000, Score: 21, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 52/1000, Score: 18, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 53/1000, Score: 20, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 54/1000, Score: 18, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 55/1000, Score: 24, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 56/1000, Score: 31, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 57/1000, Score: 18, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 58/1000, Score: 42, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 59/1000, Score: 25, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 60/1000, Score: 28, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 61/1000, Score: 35, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 62/1000, Score: 36, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 63/1000, Score: 47, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 64/1000, Score: 46, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 65/1000, Score: 34, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 66/1000, Score: 41, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 67/1000, Score: 43, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 68/1000, Score: 62, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 69/1000, Score: 82, Epsilon: 0.00997, Top score: 98\n",
      "\n",
      "Episode: 70/1000, Score: 106, Epsilon: 0.00997, Top score: 106\n",
      "\n",
      "Episode: 71/1000, Score: 150, Epsilon: 0.00997, Top score: 150\n",
      "\n",
      "Episode: 72/1000, Score: 237, Epsilon: 0.00997, Top score: 237\n",
      "\n",
      "Episode: 73/1000, Score: 137, Epsilon: 0.00997, Top score: 237\n",
      "\n",
      "Episode: 74/1000, Score: 233, Epsilon: 0.00997, Top score: 237\n",
      "\n",
      "Average score:  71.12\n",
      "\n",
      "Episode: 75/1000, Score: 244, Epsilon: 0.00997, Top score: 244\n",
      "\n",
      "Episode: 76/1000, Score: 183, Epsilon: 0.00997, Top score: 244\n",
      "\n",
      "Episode: 77/1000, Score: 192, Epsilon: 0.00997, Top score: 244\n",
      "\n",
      "Episode: 78/1000, Score: 196, Epsilon: 0.00997, Top score: 244\n",
      "\n",
      "Episode: 79/1000, Score: 282, Epsilon: 0.00997, Top score: 282\n",
      "\n",
      "Episode: 80/1000, Score: 190, Epsilon: 0.00997, Top score: 282\n",
      "\n",
      "Episode: 81/1000, Score: 183, Epsilon: 0.00997, Top score: 282\n",
      "\n",
      "Episode: 82/1000, Score: 236, Epsilon: 0.00997, Top score: 282\n",
      "\n",
      "Episode: 83/1000, Score: 299, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 84/1000, Score: 231, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 85/1000, Score: 200, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 86/1000, Score: 188, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 87/1000, Score: 202, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 88/1000, Score: 199, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 89/1000, Score: 202, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 90/1000, Score: 167, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 91/1000, Score: 194, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 92/1000, Score: 180, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 93/1000, Score: 177, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 94/1000, Score: 181, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 95/1000, Score: 187, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 96/1000, Score: 166, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 97/1000, Score: 184, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 98/1000, Score: 163, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 99/1000, Score: 168, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Average score:  197.92\n",
      "\n",
      "Episode: 100/1000, Score: 198, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 101/1000, Score: 152, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 102/1000, Score: 157, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 103/1000, Score: 144, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 104/1000, Score: 168, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 105/1000, Score: 151, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 106/1000, Score: 175, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 107/1000, Score: 157, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 108/1000, Score: 169, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 109/1000, Score: 167, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 110/1000, Score: 178, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 111/1000, Score: 154, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 112/1000, Score: 181, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 113/1000, Score: 153, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 114/1000, Score: 176, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 115/1000, Score: 185, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 116/1000, Score: 174, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 117/1000, Score: 183, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 118/1000, Score: 171, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 119/1000, Score: 151, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 120/1000, Score: 164, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 121/1000, Score: 150, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 122/1000, Score: 198, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 123/1000, Score: 11, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 124/1000, Score: 8, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Average score:  147.44\n",
      "\n",
      "Episode: 125/1000, Score: 9, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 126/1000, Score: 9, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 127/1000, Score: 11, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 128/1000, Score: 9, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 129/1000, Score: 7, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 130/1000, Score: 9, Epsilon: 0.00997, Top score: 299\n",
      "\n",
      "Episode: 131/1000, Score: 348, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 132/1000, Score: 170, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 133/1000, Score: 187, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 134/1000, Score: 155, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 135/1000, Score: 150, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 136/1000, Score: 123, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 137/1000, Score: 138, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 138/1000, Score: 150, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 139/1000, Score: 148, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 140/1000, Score: 188, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 141/1000, Score: 152, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 142/1000, Score: 193, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 143/1000, Score: 207, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 144/1000, Score: 164, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 145/1000, Score: 162, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 146/1000, Score: 162, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 147/1000, Score: 156, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 148/1000, Score: 160, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 149/1000, Score: 214, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Average score:  146.88\n",
      "\n",
      "Episode: 150/1000, Score: 300, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 151/1000, Score: 191, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 152/1000, Score: 177, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 153/1000, Score: 258, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 154/1000, Score: 194, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 155/1000, Score: 169, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 156/1000, Score: 229, Epsilon: 0.00997, Top score: 348\n",
      "\n",
      "Episode: 157/1000, Score: 162, Epsilon: 0.00997, Top score: 348\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "load_weights = False\n",
    "results = []\n",
    "\n",
    "agent = build_Deep_Q_Agent()\n",
    "\n",
    "if load_weights:\n",
    "    load(agent, 'weights/cartpole-dqn-50.h5')\n",
    "    epsilon = 0.01\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for frame in range(episodes):\n",
    "        env.render()\n",
    "        #env.reset()\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # Select action - combine prediction and randomness\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done: reward = -1\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Move to next state and save transition in memory\n",
    "        # ---------------------------------------------------------\n",
    "            \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done) \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            results.append(frame)\n",
    "            \n",
    "            if episode % 25 == 0: print(\"\\nAverage score: \", sum(results[-25:]) / 25.0)\n",
    "            if frame > max_score: max_score = frame\n",
    "            \n",
    "            print(\"\\nEpisode: {}/{}, Score: {}, Epsilon: {:.3}, Top score: {}\".format(episode, episodes, frame, float(epsilon), max_score))\n",
    "            \n",
    "            break\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Sample random minibatches from memory and train model to \n",
    "        # maximize future reward from current state\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            epsilon = replay(batch_size, epsilon, done)\n",
    "            \n",
    "    if done and frame >= 450:\n",
    "        print(\"Victory!\")\n",
    "        break\n",
    "\n",
    "    #if episode % 25 == 0:\n",
    "    #    print(\"\\nSave weights: \" + str(episode))\n",
    "    #    save(agent, \"weights/cartpole-dqn-\" + str(episode) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
