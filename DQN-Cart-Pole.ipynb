{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style=\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "**STATE**\n",
    "\n",
    "* Cart Position\n",
    "* Cart Velocity\n",
    "* Pole Angle\n",
    "* Pole Velocity at tip\n",
    "\n",
    "**ACTIONS**\n",
    "* 0 - LEFT\n",
    "* 1 - RIGHT\n",
    "\n",
    "**REWARD**\n",
    "\n",
    "* 1 for every survived step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Deep_Q_Agent():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(8, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear')) # LEFT OR RIGHT\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform action from given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(agent, state, epsilon):\n",
    "    action = -1\n",
    "    \n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = random.randrange(2)\n",
    "    else:\n",
    "        # max(Q(s,a)) - State-action function\n",
    "        action = agent.predict(state)\n",
    "        action = np.argmax(action[0]) # [0.74, 0.2] LEFT / RIGHT\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done)) # Replay memory, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent to maximize future reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size, epsilon, done):\n",
    "    \n",
    "    # Get a minibatch of previous training memories\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    \n",
    "    # For each memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        \n",
    "        if not done:\n",
    "            # Predict from next_state the (maximal) FUTURE DISCOUNTED REWARD\n",
    "            next_state_pred_reward = (reward + gamma * np.amax(agent.predict(next_state)[0]))\n",
    "        else:\n",
    "            next_state_pred_reward = reward\n",
    "        \n",
    "        \n",
    "        # Predict from current state the FUTURE DISCOUNTED REWARD depeding on action which was taken during training\n",
    "        current_state_pred_reward = agent.predict(state) # Predicted rewards [22.2112, 20.22112]\n",
    "        current_state_pred_reward[0][action] = next_state_pred_reward # Set PREDICTED FUTURE REWARD for the action which was taken\n",
    "        \n",
    "        # next_state_pred_reward = [36] (MAX, LEFT)\n",
    "        # current_state_pred_reward = [25, 23]\n",
    "        # current_state_pred_reward[action] = [36, 23]\n",
    "        \n",
    "        # This will make the agent learn the FUTURE DISCOUNTED REWARD based on current state\n",
    "        agent.fit(state, current_state_pred_reward, epochs=1, verbose=0)\n",
    "    \n",
    "    # Decrease exploration successively\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model, name):\n",
    "    return model.load_weights(name)\n",
    "\n",
    "def save(model, name):\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "memory = deque(maxlen=10000) # D-memory (transitions)\n",
    "gamma = 0.95 # discount rate\n",
    "epsilon = 0.5 # exploration rate\n",
    "epsilon_min = 0.01 # min exploration rate\n",
    "epsilon_decay = 0.995 # decrease exploration successively\n",
    "learning_rate = 0.0005\n",
    "episodes = 1000\n",
    "batch_size = 25\n",
    "max_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(range(len(results)), results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average score:  0.96\n",
      "\n",
      "Episode: 0/1000, Score: 24, Epsilon: 0.5, Top score: 24\n",
      "\n",
      "Episode: 1/1000, Score: 29, Epsilon: 0.432, Top score: 29\n",
      "\n",
      "Episode: 2/1000, Score: 57, Epsilon: 0.325, Top score: 57\n",
      "\n",
      "Episode: 3/1000, Score: 18, Epsilon: 0.297, Top score: 57\n",
      "\n",
      "Episode: 4/1000, Score: 20, Epsilon: 0.269, Top score: 57\n",
      "\n",
      "Episode: 5/1000, Score: 25, Epsilon: 0.237, Top score: 57\n",
      "\n",
      "Episode: 6/1000, Score: 30, Epsilon: 0.204, Top score: 57\n",
      "\n",
      "Episode: 7/1000, Score: 25, Epsilon: 0.18, Top score: 57\n",
      "\n",
      "Episode: 8/1000, Score: 41, Epsilon: 0.146, Top score: 57\n",
      "\n",
      "Episode: 9/1000, Score: 71, Epsilon: 0.103, Top score: 71\n",
      "\n",
      "Episode: 10/1000, Score: 88, Epsilon: 0.066, Top score: 88\n",
      "\n",
      "Episode: 11/1000, Score: 77, Epsilon: 0.0449, Top score: 88\n",
      "\n",
      "Episode: 12/1000, Score: 126, Epsilon: 0.0239, Top score: 126\n",
      "\n",
      "Episode: 13/1000, Score: 160, Epsilon: 0.0107, Top score: 160\n",
      "\n",
      "Episode: 14/1000, Score: 202, Epsilon: 0.00997, Top score: 202\n",
      "\n",
      "Episode: 15/1000, Score: 130, Epsilon: 0.00997, Top score: 202\n",
      "\n",
      "Episode: 16/1000, Score: 257, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 17/1000, Score: 196, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 18/1000, Score: 225, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 19/1000, Score: 141, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 20/1000, Score: 249, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 21/1000, Score: 97, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 22/1000, Score: 112, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 23/1000, Score: 120, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 24/1000, Score: 171, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Average score:  110.76\n",
      "\n",
      "Episode: 25/1000, Score: 102, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 26/1000, Score: 83, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 27/1000, Score: 112, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 28/1000, Score: 229, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 29/1000, Score: 158, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 30/1000, Score: 100, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 31/1000, Score: 87, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 32/1000, Score: 142, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 33/1000, Score: 88, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 34/1000, Score: 124, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 35/1000, Score: 135, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 36/1000, Score: 203, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 37/1000, Score: 168, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 38/1000, Score: 77, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 39/1000, Score: 180, Epsilon: 0.00997, Top score: 257\n",
      "\n",
      "Episode: 40/1000, Score: 259, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 41/1000, Score: 246, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 42/1000, Score: 100, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 43/1000, Score: 173, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 44/1000, Score: 161, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 45/1000, Score: 101, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 46/1000, Score: 150, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 47/1000, Score: 158, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 48/1000, Score: 141, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 49/1000, Score: 147, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Average score:  147.84\n",
      "\n",
      "Episode: 50/1000, Score: 174, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 51/1000, Score: 169, Epsilon: 0.00997, Top score: 259\n",
      "\n",
      "Episode: 52/1000, Score: 265, Epsilon: 0.00997, Top score: 265\n",
      "\n",
      "Episode: 53/1000, Score: 173, Epsilon: 0.00997, Top score: 265\n",
      "\n",
      "Episode: 54/1000, Score: 131, Epsilon: 0.00997, Top score: 265\n",
      "\n",
      "Episode: 55/1000, Score: 146, Epsilon: 0.00997, Top score: 265\n",
      "\n",
      "Episode: 56/1000, Score: 336, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 57/1000, Score: 170, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 58/1000, Score: 121, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 59/1000, Score: 126, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 60/1000, Score: 96, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 61/1000, Score: 110, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 62/1000, Score: 84, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 63/1000, Score: 118, Epsilon: 0.00997, Top score: 336\n",
      "\n",
      "Episode: 64/1000, Score: 365, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 65/1000, Score: 350, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 66/1000, Score: 178, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 67/1000, Score: 218, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 68/1000, Score: 281, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 69/1000, Score: 169, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 70/1000, Score: 175, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 71/1000, Score: 163, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 72/1000, Score: 241, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 73/1000, Score: 213, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 74/1000, Score: 11, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Average score:  183.4\n",
      "\n",
      "Episode: 75/1000, Score: 176, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 76/1000, Score: 80, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 77/1000, Score: 210, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 78/1000, Score: 65, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 79/1000, Score: 10, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 80/1000, Score: 11, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 81/1000, Score: 12, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 82/1000, Score: 142, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 83/1000, Score: 153, Epsilon: 0.00997, Top score: 365\n",
      "\n",
      "Episode: 84/1000, Score: 409, Epsilon: 0.00997, Top score: 409\n",
      "\n",
      "Episode: 85/1000, Score: 188, Epsilon: 0.00997, Top score: 409\n",
      "\n",
      "Episode: 86/1000, Score: 499, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 87/1000, Score: 46, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 88/1000, Score: 405, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 89/1000, Score: 251, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 90/1000, Score: 50, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 91/1000, Score: 94, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 92/1000, Score: 163, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 93/1000, Score: 81, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 94/1000, Score: 267, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 95/1000, Score: 8, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 96/1000, Score: 9, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 97/1000, Score: 9, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 98/1000, Score: 8, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 99/1000, Score: 11, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Average score:  133.92\n",
      "\n",
      "Episode: 100/1000, Score: 167, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 101/1000, Score: 81, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 102/1000, Score: 77, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 103/1000, Score: 52, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 104/1000, Score: 89, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 105/1000, Score: 128, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 106/1000, Score: 116, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 107/1000, Score: 121, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 108/1000, Score: 128, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 109/1000, Score: 108, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 110/1000, Score: 154, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 111/1000, Score: 139, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 112/1000, Score: 198, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 113/1000, Score: 268, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 114/1000, Score: 278, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 115/1000, Score: 234, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 116/1000, Score: 190, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 117/1000, Score: 163, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 118/1000, Score: 169, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 119/1000, Score: 166, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 120/1000, Score: 178, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 121/1000, Score: 380, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 122/1000, Score: 286, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 123/1000, Score: 141, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 124/1000, Score: 186, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Average score:  168.32\n",
      "\n",
      "Episode: 125/1000, Score: 178, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 126/1000, Score: 146, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 127/1000, Score: 238, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 128/1000, Score: 143, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 129/1000, Score: 175, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 130/1000, Score: 235, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 131/1000, Score: 235, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 132/1000, Score: 192, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 133/1000, Score: 129, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 134/1000, Score: 178, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 135/1000, Score: 277, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 136/1000, Score: 210, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 137/1000, Score: 200, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 138/1000, Score: 176, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 139/1000, Score: 195, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 140/1000, Score: 253, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 141/1000, Score: 208, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 142/1000, Score: 128, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 143/1000, Score: 216, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 144/1000, Score: 146, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 145/1000, Score: 116, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 146/1000, Score: 148, Epsilon: 0.00997, Top score: 499\n",
      "\n",
      "Episode: 147/1000, Score: 173, Epsilon: 0.00997, Top score: 499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b982daa25223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#if episode % 25 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e8d6a0b1f945>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(batch_size, epsilon, done)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# This will make the agent learn the FUTURE DISCOUNTED REWARD based on current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state_pred_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Decrease exploration successively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "load_weights = False\n",
    "results = []\n",
    "\n",
    "agent = build_Deep_Q_Agent()\n",
    "\n",
    "if load_weights:\n",
    "    load(agent, 'weights/cartpole-dqn-50.h5')\n",
    "    epsilon = 0.01\n",
    "\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for frame in range(episodes):\n",
    "        env.render()\n",
    "        #env.reset()\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # Select action - combine prediction and randomness\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        action = perform_action(agent, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done: reward = -1\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Move to next state and save transition in memory\n",
    "        # ---------------------------------------------------------\n",
    "            \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done) \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            results.append(frame)\n",
    "            \n",
    "            if episode % 25 == 0: print(\"\\nAverage score: \", sum(results[-25:]) / 25.0)\n",
    "            if frame > max_score: max_score = frame\n",
    "            \n",
    "            print(\"\\nEpisode: {}/{}, Score: {}, Epsilon: {:.3}, Top score: {}\".format(episode, episodes, frame, float(epsilon), max_score))\n",
    "            \n",
    "            break\n",
    "            \n",
    "        # ---------------------------------------------------------\n",
    "        # Sample random minibatches from memory and train model to \n",
    "        # maximize future reward from current state\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            epsilon = replay(batch_size, epsilon, done)\n",
    "\n",
    "    #if episode % 25 == 0:\n",
    "    #    print(\"\\nSave weights: \" + str(episode))\n",
    "    #    save(agent, \"weights/cartpole-dqn-\" + str(episode) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
