{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation samples for scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_samples = []\n",
    "\n",
    "for n in range(100):\n",
    "    observation = env.reset()\n",
    "    observation_samples.append(observation)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation_samples.append(observation)\n",
    "        \n",
    "observation_samples = np.array(observation_samples)\n",
    "\n",
    "# Create scaler and fit\n",
    "sc = StandardScaler()\n",
    "sc.fit(observation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation sample\n",
    "#\n",
    "# array([ 0.00379438,  1.4016738 ,  0.38431674, -0.41094953, -0.00438996,\n",
    "#       -0.08705341,  0.        ,  0.        ], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = wrappers.Monitor(env, 'monitor-folder', force=True) # Saves cubed episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Networks - One for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='uniform', input_shape=(8,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_initializer='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform'))\n",
    "    model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, env, scaler):\n",
    "        self.env = env\n",
    "        self.scaler = scaler\n",
    "        self.models = []\n",
    "        for i in range(env.action_space.n):\n",
    "            model = build_neural_network()\n",
    "            self.models.append(model) \n",
    "\n",
    "    def predict(self, state):\n",
    "        state = self.scaler.transform(np.atleast_2d(state))\n",
    "        preds = []\n",
    "        for m in self.models:\n",
    "            preds.append(m.predict(np.array(state), verbose=0)[0])\n",
    "        return np.array(preds)                 \n",
    "\n",
    "    def _fit(self, state, action, future_discounted_reward):\n",
    "        state = self.scaler.transform(np.atleast_2d(state))\n",
    "        model = self.models[action] # Different model depending on action\n",
    "        \n",
    "        #print(type(future_discounted_reward))\n",
    "        \n",
    "        if type(future_discounted_reward) is np.ndarray:\n",
    "            print(future_discounted_reward)\n",
    "            #future_dsicounted_reward = future_discounted_reward[0][action]\n",
    "            x = 1\n",
    "            print(np.array(future_discounted_reward[0][action]).shape)\n",
    "            print(np.array(state).shape)\n",
    "            model.fit(np.array(state), \n",
    "                      np.array(future_discounted_reward[0][action]), \n",
    "                      epochs=1, verbose=0)\n",
    "        else:\n",
    "            #future_dsicounted_reward = future_discounted_reward[action]\n",
    "            print(future_discounted_reward)\n",
    "            model.fit(np.array(state), np.array([future_discounted_reward]), epochs=1, verbose=0)\n",
    "        #future_discounted_reward = future_discounted_reward[action]\n",
    "        #print(\"State: \", state)\n",
    "        #print(\"Reward: \", [future_discounted_reward])\n",
    "   \n",
    "\n",
    "    def action(self, state, EPSILON):\n",
    "        if np.random.random() < EPSILON:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.predict(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(prev_state, action, reward, state, done):\n",
    "    memory.append((prev_state, action, reward, state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(memory, BATCH_SIZE):\n",
    "    mini_batch = random.sample(memory, 100) # MAX 2000 in memory\n",
    "    \n",
    "    for prev_state, action, reward, state, done in mini_batch:\n",
    "        if not done:\n",
    "            next_state_pred_reward = reward + GAMMA * np.max(model.predict(state)[0])\n",
    "        else:\n",
    "            next_state_pred_reward = reward\n",
    "        \n",
    "        \n",
    "        # PREDICT = INPUT ONE STATE -> 4 REWARDS (ARGMAX FOR ACTION)\n",
    "        # FIT = \n",
    "        \n",
    "        '''\n",
    "    next_state_pred_reward:  -99.82350345313549\n",
    "    \n",
    "    state_pred_reward [[-0.21892993]\n",
    "     [-0.12000708]\n",
    "     [-0.14074107]\n",
    "     [-0.5067307 ]]\n",
    "    (1, 4)\n",
    "\n",
    "    State:  [[-0.5119985  -2.050301    0.45516843 -0.79553884 -2.557886   -8.063909\n",
    "      -0.12273416 -0.16050309]]\n",
    "              \n",
    "        '''\n",
    "        \n",
    "        #print(\"next_state_pred_reward: \", next_state_pred_reward)\n",
    "        state_pred_reward = model.predict(state)\n",
    "        #print(\"state_pred_reward\", state_pred_reward)\n",
    "        state_pred_reward = state_pred_reward.reshape(1, -1)\n",
    "        state_pred_reward[0][action] = next_state_pred_reward\n",
    "        #print(\"state_pred_reward_after\", state_pred_reward)\n",
    "        print(state_pred_reward.shape)\n",
    "        model._fit(prev_state, action, state_pred_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(episode_rewards):\n",
    "    running_avg = np.empty(len(episode_rewards))\n",
    "    running_avg = list(map(lambda t: episode_rewards[max(0, t-25):(t+1)].mean(), range(len(episode_rewards))))\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mission_accomplished(episode_rewards, e):\n",
    "    return episode_rewards[max(0, e-100):(e+1)].mean() >= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "GAMMA = 0.99 # DISCOUNT\n",
    "EPSILON = 1.0 / np.sqrt(1) # EXPLORATION RATE\n",
    "EPSILON_DECAY_RATE = 0.001\n",
    "EPSILON_MIN = 0.001\n",
    "BATCH_SIZE = 10\n",
    "episode_rewards = np.empty(1000)\n",
    "from collections import deque\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41557726110863996\n",
      "-0.4334262347177867\n",
      "1.2328217117343059\n",
      "0.43095975843357903\n",
      "-0.8917287965253207\n",
      "-0.7778270502423675\n",
      "-0.9787182979051022\n",
      "-1.4755010298476408\n",
      "0.7339660480842497\n",
      "-1.4438465780456682\n",
      "0.4072372838367141\n",
      "-1.1602478694177842\n",
      "-0.5392656202201636\n",
      "-1.6085622715777868\n",
      "-0.7618545310712797\n",
      "-0.9579490221675939\n",
      "-1.0567628823166673\n",
      "-1.1574494336772125\n",
      "-3.564872806610997\n",
      "0.08758163849293737\n",
      "-3.486787598099281\n",
      "-2.2788145398988613\n",
      "-2.4602990245187857\n",
      "-2.0997673500730865\n",
      "-1.4713679701141722\n",
      "-0.2251665478958603\n",
      "-0.1237736112109178\n",
      "-0.3344470129015213\n",
      "0.18918204378104064\n",
      "-1.0219015984186626\n",
      "0.09021891246642554\n",
      "-0.9425880882767274\n",
      "-2.0478856438784176\n",
      "-1.207683733807106\n",
      "-2.359453840506935\n",
      "-0.6565460484875257\n",
      "-0.6407305800536562\n",
      "-2.6566328791110663\n",
      "-2.6223063039920733\n",
      "-1.7424252000286016\n",
      "-0.8345233671905249\n",
      "-2.6422714961831137\n",
      "-1.9104223068799218\n",
      "-0.8168740259279605\n",
      "-2.795548079828062\n",
      "-1.0883781967256563\n",
      "-2.6749572357207567\n",
      "-2.6999763193178876\n",
      "-1.7194133915140084\n",
      "-2.9103102251096717\n",
      "-2.14903094826014\n",
      "-0.8842049142856536\n",
      "-1.06324182916062\n",
      "-0.869330558580964\n",
      "-2.530026305922263\n",
      "-1.6962466008148704\n",
      "-2.5132361432360697\n",
      "-1.8169356613366585\n",
      "-1.7398813080134665\n",
      "-2.6226157203364524\n",
      "-2.701539781824167\n",
      "-2.7833133633931424\n",
      "-3.1256721179972735\n",
      "-2.845611555154198\n",
      "-2.987589613621005\n",
      "-2.279902052432135\n",
      "-2.3089909706765295\n",
      "-2.3460181331225543\n",
      "-2.3392690699386094\n",
      "-2.333232002082582\n",
      "-1.339253516312108\n",
      "-1.25022260571526\n",
      "-3.319426863559572\n",
      "-1.9088550316096098\n",
      "-1.9122379197769124\n",
      "-0.7751440153333533\n",
      "-2.9705070692991784\n",
      "-1.9642237905568505\n",
      "-0.886093395059304\n",
      "-4.542873575537978\n",
      "-3.485044266333084\n",
      "-3.0578388849443034\n",
      "-2.1223227134256466\n",
      "-1.0032334223534145\n",
      "-1.1581891118609293\n",
      "-2.833702998579019\n",
      "-1.22768149564876\n",
      "-1.2888625583136855\n",
      "-3.025948737143824\n",
      "-4.8607401931529575\n",
      "-3.4965481641266773\n",
      "-3.7800240312215734\n",
      "-4.898418964141963\n",
      "-5.545281008139625\n",
      "5.5756756857459955\n",
      "10.217784861617062\n",
      "-100.080989824906\n",
      "97\n",
      "Episode:  0 Itr 97 Reward: -254.01011680482466 Epsilon: 1.000 Avg reward (25): -254.01011680482466\n",
      "-1.683074316895145\n",
      "0.26201860359657525\n",
      "0.4986803445843543\n",
      "-0.3675621228210593\n",
      "0.6715574173203709\n",
      "0.8450489923641055\n",
      "-3.004624521068267\n",
      "0.010505457839631163\n",
      "-1.0677019484596884\n",
      "-0.23950766710056542\n",
      "-0.3852963318760021\n",
      "-0.32573787632562473\n",
      "0.9232280377809808\n",
      "-1.15232763654883\n",
      "-1.8075147520337997\n",
      "0.4614329205558306\n",
      "-0.5088615364709245\n",
      "-1.5969604521814063\n",
      "-0.7672568137314465\n",
      "-0.8090994694407072\n",
      "0.1769372209401422\n",
      "-0.7097698211921852\n",
      "-0.7486262858755299\n",
      "0.23259111294076662\n",
      "-1.990086735257156\n",
      "-0.10720653745505274\n",
      "-0.9271383144249024\n",
      "-1.9699653619725224\n",
      "0.03811507276204907\n",
      "0.09246334640354235\n",
      "-2.397248418289731\n",
      "-2.1520126904433865\n",
      "-0.32628540229884606\n",
      "-0.028876123111211884\n",
      "-0.006748455872314027\n",
      "0.11257912778654372\n",
      "0.2317195786991999\n",
      "-0.5463275740998926\n",
      "0.4089658554109246\n",
      "0.8649181722012826\n",
      "1.0310722103983574\n",
      "0.8571580696643457\n",
      "-0.8863226934611373\n",
      "0.9814380019075577\n",
      "0.24003647894884808\n",
      "-0.7814171405324487\n",
      "1.0643010530179822\n",
      "1.3737024771828805\n",
      "1.477794251462634\n",
      "2.8804965772854203\n",
      "1.783396704728716\n",
      "0.9704795678634446\n",
      "1.7867717297571801\n",
      "1.8041576013093266\n",
      "2.339931110547607\n",
      "-1.6926475904768472\n",
      "3.070780653368188\n",
      "-2.836607675248863\n",
      "1.4992349450931919\n",
      "-2.611997542649351\n",
      "-3.1461796392266477\n",
      "-2.9277038294986584\n",
      "-3.5315640486980757\n",
      "-0.8296461871017972\n",
      "0.8466428914094717\n",
      "-3.110991607061354\n",
      "-3.429540125242357\n",
      "-3.373933076838527\n",
      "-3.741459470398115\n",
      "-3.5248878289654884\n",
      "-3.707048706107921\n",
      "-4.01138814239532\n",
      "-4.249532617179991\n",
      "-4.516888440692999\n",
      "-4.35519804075924\n",
      "-4.396561435472665\n",
      "-4.794608271642308\n",
      "-1.9020096413884995\n",
      "0.7864216697020652\n",
      "-4.57275706043266\n",
      "0.43232333346098806\n",
      "-5.310663097811114\n",
      "-5.586273571963898\n",
      "-5.000892311622804\n",
      "-4.832415375179533\n",
      "-4.487196195841084\n",
      "-2.840750533678442\n",
      "-5.810403307395764\n",
      "-6.126528063350059\n",
      "8.019305537109664\n",
      "49.671205139804734\n",
      "-33.10714336369989\n",
      "-21.591181330967586\n",
      "-100.09159008525312\n",
      "191\n",
      "0.4918403847315801\n",
      "-1.5586808085505925\n",
      "-2.074153331561865\n",
      "0.21681372008516975\n",
      "-1.601956421889995\n",
      "-4.026004419580313\n",
      "-1.8542626943413962\n",
      "-0.9339066608755195\n",
      "0.3257650830240129\n",
      "0.24563224532387723\n",
      "-4.1811996929886\n",
      "-1.8788851996227982\n",
      "-2.348038537367987\n",
      "0.3341748396152809\n",
      "0.795726499202583\n",
      "-1.7187602946651601\n",
      "0.978773524593563\n",
      "0.7620319333544774\n",
      "1.0311259549666851\n",
      "1.2366415651719012\n",
      "0.26654761379790215\n",
      "-0.11417142314281135\n",
      "-0.9921983854813221\n",
      "-1.0819780675117319\n",
      "1.0312461467475826\n",
      "0.09894129721204536\n",
      "-1.2243999439712707\n",
      "0.8622684020138968\n",
      "0.9623231466013124\n",
      "-1.2530375270289187\n",
      "1.1295647538791547\n",
      "-1.0804802253290609\n",
      "0.05740348520263524\n",
      "0.026455131374468692\n",
      "0.7632630112716412\n",
      "1.174921203346438\n",
      "0.9882833268887055\n",
      "1.3954336673939371\n",
      "0.45616486280864477\n",
      "1.2720191208874905\n",
      "2.686813056688159\n",
      "-0.5161837194005295\n",
      "2.1654714509153896\n",
      "1.6730326384827026\n",
      "1.730582531809423\n",
      "-0.4590718743553566\n",
      "0.07350915577093702\n",
      "-0.8006227813663668\n",
      "-1.064614517119494\n",
      "-1.783968497677787\n",
      "-1.802703415986374\n",
      "-1.2905595097476839\n",
      "0.03177660492586939\n",
      "-1.5415602566362248\n",
      "-2.31795312090936\n",
      "-1.8303807823263663\n",
      "-2.0938482122717605\n",
      "-1.8001584218379878\n",
      "-3.4342702731760046\n",
      "3.4630177866487433\n",
      "-2.213376176565367\n",
      "-1.707313735418886\n",
      "-3.5057040665465933\n",
      "-2.3756364667118635\n",
      "-2.9154662738994714\n",
      "-2.9203647911064103\n",
      "-2.924276011967647\n",
      "-2.6704667986291564\n",
      "-3.423955898062636\n",
      "-2.6990963981764025\n",
      "-3.056008966120057\n",
      "-3.4230960837991002\n",
      "-3.1557218251976087\n",
      "1.4402492623556848\n",
      "3.6418059270505094\n",
      "-3.154371007506427\n",
      "-3.417454118147034\n",
      "-3.5160090517147613\n",
      "-3.887082107091896\n",
      "-4.159156792906392\n",
      "-4.150325628684695\n",
      "-4.031641773683739\n",
      "-3.96511433714757\n",
      "-4.2751082939197085\n",
      "-4.553251531367614\n",
      "-0.3020697254887477\n",
      "-4.591655002828422\n",
      "-4.980301624856318\n",
      "-5.383553795686353\n",
      "-5.15095116339208\n",
      "-4.644427500210104\n",
      "-5.5633730419150105\n",
      "0.9521676771974625\n",
      "-4.809776313646626\n",
      "-5.213326328718445\n",
      "-2.321092250410511\n",
      "-1.3205948618262937\n",
      "-3.0630051781320753\n",
      "-2.7120362959930184\n",
      "-4.838613846819246\n",
      "-6.57706416900681\n",
      "-5.422812349500179\n",
      "-6.816294305386955\n",
      "-7.397343751195544\n",
      "-7.738817052322311\n",
      "-5.684292857544918\n",
      "-6.607323226031092\n",
      "-6.784931338786387\n",
      "-6.973286491617935\n",
      "-8.944572438139922\n",
      "-7.929249525999298\n",
      "-100.39035816431046\n",
      "303\n",
      "Replay!\n",
      "(1, 4)\n",
      "[[-1.4255428   0.03571498 -0.7404695  -2.2381654 ]]\n",
      "()\n",
      "(1, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_3 to have 2 dimensions, but got array with shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f6e29f2e6281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Replay!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dafd2cb44463>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(memory, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#print(\"state_pred_reward_after\", state_pred_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_pred_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_pred_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-09842266113a>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, state, action, future_discounted_reward)\u001b[0m\n\u001b[1;32m     30\u001b[0m             model.fit(np.array(state), \n\u001b[1;32m     31\u001b[0m                       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_discounted_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                       epochs=1, verbose=0)\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#future_dsicounted_reward = future_discounted_reward[action]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_3 to have 2 dimensions, but got array with shape ()"
     ]
    }
   ],
   "source": [
    "model = Model(env, sc)\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    \n",
    "    EPSILON = 1.0 / np.sqrt(e+1)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    frames = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Perform action\n",
    "        \n",
    "        #env.render()\n",
    "        action = model.action(state, EPSILON)\n",
    "        prev_state = state\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Predict future discounted reward from current state and train model to find this mapping\n",
    "\n",
    "        pred_reward = model.predict(state)\n",
    "        future_discounted_reward = reward + GAMMA * np.max(pred_reward)\n",
    "        model._fit(prev_state, action, future_discounted_reward)\n",
    "        episode_reward += reward\n",
    "        frames += 1\n",
    "        \n",
    "        remember(prev_state, action, future_discounted_reward, state, done)\n",
    "        \n",
    "    episode_rewards[e] = episode_reward\n",
    "    \n",
    "    print(len(memory))\n",
    "    if len(memory) > 250: # 250\n",
    "        print(\"Replay!\")\n",
    "        replay(memory, BATCH_SIZE)\n",
    "    \n",
    "    if len(memory) >= 1000:\n",
    "        print(\"Clear!\")\n",
    "        memory.clear()\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print(\"Episode: \", e, \"Itr\", frames, \"Reward:\", episode_reward, \"Epsilon: %.3f\" % EPSILON, \"Avg reward (25):\", episode_rewards[max(0, e-25):(e+1)].mean())\n",
    "    if mission_accomplished(episode_rewards, e):\n",
    "        break\n",
    "\n",
    "print(\"Avg reward for last 100 episodes:\", episode_rewards[-100:].mean())\n",
    "print(\"Total steps:\", episode_rewards.sum())\n",
    "\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
