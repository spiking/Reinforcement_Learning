{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation samples for scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_samples = []\n",
    "\n",
    "for n in range(100):\n",
    "    observation = env.reset()\n",
    "    observation_samples.append(observation)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation_samples.append(observation)\n",
    "        \n",
    "observation_samples = np.array(observation_samples)\n",
    "\n",
    "# Create scaler and fit\n",
    "sc = StandardScaler()\n",
    "sc.fit(observation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation sample\n",
    "#\n",
    "# array([ 0.00379438,  1.4016738 ,  0.38431674, -0.41094953, -0.00438996,\n",
    "#       -0.08705341,  0.        ,  0.        ], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = wrappers.Monitor(env, 'monitor-folder', force=True) # Saves cubed episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Networks - One for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, kernel_initializer='uniform', input_shape=(8,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(256, kernel_initializer='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform'))\n",
    "    model.add(Activation('linear')) #linear output so we can have range of real-valued outputs\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adamax')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, env, scaler):\n",
    "        self.env = env\n",
    "        self.scaler = scaler\n",
    "        self.models = []\n",
    "        for i in range(env.action_space.n):\n",
    "            model = build_neural_network()\n",
    "            self.models.append(model) \n",
    "\n",
    "    def predict(self, state):\n",
    "        state = self.scaler.transform(np.atleast_2d(state))\n",
    "        preds = []\n",
    "        for m in self.models:\n",
    "            preds.append(m.predict(np.array(state), verbose=0)[0])\n",
    "        return np.array(preds)                 \n",
    "\n",
    "    def _fit(self, state, action, future_discounted_reward):\n",
    "        state = self.scaler.transform(np.atleast_2d(state))\n",
    "        model = self.models[action] # Different model depending on action\n",
    "        \n",
    "        #print(type(future_discounted_reward))\n",
    "        \n",
    "        if type(future_discounted_reward) is np.ndarray:\n",
    "            print(future_discounted_reward)\n",
    "            #future_dsicounted_reward = future_discounted_reward[0][action]\n",
    "            x = 1\n",
    "            print(np.array(future_discounted_reward[0][action]).shape)\n",
    "            print(np.array(state).shape)\n",
    "            model.fit(np.array(state), \n",
    "                      np.array(future_discounted_reward[0][action]), \n",
    "                      epochs=1, verbose=0)\n",
    "        else:\n",
    "            #future_dsicounted_reward = future_discounted_reward[action]\n",
    "            print(future_discounted_reward)\n",
    "            model.fit(np.array(state), np.array([future_discounted_reward]), epochs=1, verbose=0)\n",
    "        #future_discounted_reward = future_discounted_reward[action]\n",
    "        #print(\"State: \", state)\n",
    "        #print(\"Reward: \", [future_discounted_reward])\n",
    "   \n",
    "\n",
    "    def action(self, state, EPSILON):\n",
    "        if np.random.random() < EPSILON:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.predict(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(prev_state, action, reward, state, done):\n",
    "    memory.append((prev_state, action, reward, state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(memory, BATCH_SIZE):\n",
    "    mini_batch = random.sample(memory, 100) # MAX 2000 in memory\n",
    "    \n",
    "    for prev_state, action, reward, state, done in mini_batch:\n",
    "        if not done:\n",
    "            next_state_pred_reward = reward + GAMMA * np.max(model.predict(state)[0])\n",
    "        else:\n",
    "            next_state_pred_reward = reward\n",
    "        \n",
    "        \n",
    "        # PREDICT = INPUT ONE STATE -> 4 REWARDS (ARGMAX FOR ACTION)\n",
    "        # FIT = \n",
    "        \n",
    "        '''\n",
    "    next_state_pred_reward:  -99.82350345313549\n",
    "    \n",
    "    state_pred_reward [[-0.21892993]\n",
    "     [-0.12000708]\n",
    "     [-0.14074107]\n",
    "     [-0.5067307 ]]\n",
    "    (1, 4)\n",
    "\n",
    "    State:  [[-0.5119985  -2.050301    0.45516843 -0.79553884 -2.557886   -8.063909\n",
    "      -0.12273416 -0.16050309]]\n",
    "              \n",
    "        '''\n",
    "        \n",
    "        #print(\"next_state_pred_reward: \", next_state_pred_reward)\n",
    "        state_pred_reward = model.predict(state)\n",
    "        #print(\"state_pred_reward\", state_pred_reward)\n",
    "        state_pred_reward = state_pred_reward.reshape(1, -1)\n",
    "        state_pred_reward[0][action] = next_state_pred_reward\n",
    "        #print(\"state_pred_reward_after\", state_pred_reward)\n",
    "        print(state_pred_reward.shape)\n",
    "        model._fit(prev_state, action, state_pred_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(episode_rewards):\n",
    "    running_avg = np.empty(len(episode_rewards))\n",
    "    running_avg = list(map(lambda t: episode_rewards[max(0, t-25):(t+1)].mean(), range(len(episode_rewards))))\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mission_accomplished(episode_rewards, e):\n",
    "    return episode_rewards[max(0, e-100):(e+1)].mean() >= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "GAMMA = 0.99 # DISCOUNT\n",
    "EPSILON = 1.0 / np.sqrt(1) # EXPLORATION RATE\n",
    "EPSILON_DECAY_RATE = 0.001\n",
    "EPSILON_MIN = 0.001\n",
    "BATCH_SIZE = 10\n",
    "episode_rewards = np.empty(1000)\n",
    "from collections import deque\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.43089758648579846\n",
      "-1.5397417991988847\n",
      "-1.5203512323190704\n",
      "-2.086634413791113\n",
      "-0.01425379091426521\n",
      "0.11935966870356196\n",
      "0.1910580358810239\n",
      "-3.350988973836845\n",
      "-1.8541350128452314\n",
      "-0.794379190286927\n",
      "0.3299435696975729\n",
      "-0.2068191260982985\n",
      "0.307316468854666\n",
      "-0.5700394975650273\n",
      "-1.8023034455257494\n",
      "0.38142312991551874\n",
      "-0.6668304083550936\n",
      "0.7047614018623926\n",
      "-0.5025364729628597\n",
      "-0.8909181848943206\n",
      "-0.3993445919996111\n",
      "-2.0324370730272108\n",
      "0.567244615809459\n",
      "-0.6613359235366784\n",
      "-1.3405324104571241\n",
      "0.16843882574782357\n",
      "-3.32380933145189\n",
      "-0.416173420168044\n",
      "0.43150459150367393\n",
      "-1.2551293869086089\n",
      "-0.02101671128161569\n",
      "-0.548908348278594\n",
      "-1.5238657540763143\n",
      "0.6608510327839338\n",
      "0.5551536895986577\n",
      "-1.6561968396648388\n",
      "0.7468405481494779\n",
      "-1.3135159685497069\n",
      "-0.35936598752088683\n",
      "-1.8242678069372789\n",
      "0.6371457786936946\n",
      "0.7387851219654475\n",
      "-0.2808076982317516\n",
      "-1.6949708807974162\n",
      "-0.542401483308779\n",
      "-1.6639387532626346\n",
      "-1.7541417957407486\n",
      "-0.03748260248418887\n",
      "-1.8553271131694693\n",
      "-1.8574914167494103\n",
      "0.4734112388862002\n",
      "-0.8398716061621065\n",
      "-0.03996081453749979\n",
      "-1.1590194226000983\n",
      "-0.19490743433716262\n",
      "0.27580105667655064\n",
      "-1.5073156177893408\n",
      "0.2887549109152451\n",
      "-1.8348655330996666\n",
      "-1.6860847343226215\n",
      "-2.26792500400335\n",
      "0.5220353515633883\n",
      "-0.2527307587447536\n",
      "-1.0385949292131\n",
      "-1.043915457021376\n",
      "-1.0488951901012569\n",
      "-2.3198237829768615\n",
      "-0.9661448707479758\n",
      "0.39487329298309537\n",
      "-2.1856572424556293\n",
      "-1.2691611396782423\n",
      "-1.278290224204036\n",
      "-1.2881859062599892\n",
      "-0.051805143701492404\n",
      "0.9699289752043592\n",
      "-0.30170714912116864\n",
      "-1.2549077969956057\n",
      "-2.500309303679664\n",
      "-0.31082259052250494\n",
      "-1.6015689546829686\n",
      "-2.5089729977540376\n",
      "-3.1826940386249976\n",
      "-2.095627557188595\n",
      "-2.127614395339358\n",
      "-0.949703769232217\n",
      "-2.1670352842874783\n",
      "-3.3495534873428934\n",
      "-2.3109049365903864\n",
      "-3.427891759449194\n",
      "-2.491462419238487\n",
      "-1.9282007839262563\n",
      "-3.5433413802849736\n",
      "-2.8603498221891823\n",
      "-2.9258081305356995\n",
      "-2.8186831020622796\n",
      "-1.9877667628508946\n",
      "-1.9786072782513473\n",
      "-3.8587055762743736\n",
      "-4.8882290316702575\n",
      "-3.2409884045758757\n",
      "-4.761339152380974\n",
      "-2.5509916660251197\n",
      "-3.3383295402579187\n",
      "-4.40543485123381\n",
      "-2.762883971417509\n",
      "-4.356216279452217\n",
      "4.99458877570486\n",
      "24.181726449517612\n",
      "-22.379261612158672\n",
      "-31.21874870014116\n",
      "-99.89806536078453\n",
      "111\n",
      "Episode:  0 Itr 111 Reward: -270.4315805504941 Epsilon: 1.000 Avg reward (25): -270.4315805504941\n",
      "1.9835305012354034\n",
      "-2.3361525634970506\n",
      "-0.6587526310170881\n",
      "-2.3595248980708012\n",
      "-1.602161417401743\n",
      "-2.6745560058576108\n",
      "-2.616912559320649\n",
      "-1.9942009214806786\n",
      "1.6223277125339586\n",
      "-3.0953332134323612\n",
      "-1.6132982156537947\n",
      "-2.1108204356519367\n",
      "-2.0998165275796996\n",
      "1.1476770557170357\n",
      "-2.738547209260841\n",
      "-2.879633985335488\n",
      "-2.3289955139859306\n",
      "-1.6474237009593709\n",
      "-2.9207214648016087\n",
      "-2.995621969051466\n",
      "-3.214437602487072\n",
      "1.5322316342114675\n",
      "-2.5764378587260692\n",
      "-0.5617781214612808\n",
      "-3.5408604687112093\n",
      "-3.366797219181643\n",
      "-1.8929065707437769\n",
      "-2.534795628651841\n",
      "-0.5710612069988782\n",
      "-1.7587919245415344\n",
      "-3.3257015734448463\n",
      "-3.4469194731013473\n",
      "-3.465485570562846\n",
      "-2.8070036429020297\n",
      "-1.8212722730308064\n",
      "-2.5211463878672293\n",
      "-3.632148807848772\n",
      "-3.6487311776341684\n",
      "-1.3753400015086266\n",
      "-2.857795713022\n",
      "-2.058797188531689\n",
      "-0.5090040302842818\n",
      "-3.5080303498118086\n",
      "-3.8116189448004545\n",
      "-3.31459345208897\n",
      "-3.743319573916915\n",
      "-1.9817613936744216\n",
      "-1.4890777142095477\n",
      "-2.4023637480382174\n",
      "-2.2158443983990694\n",
      "-3.358274718678094\n",
      "-3.4439186436822453\n",
      "-3.73612446861601\n",
      "-4.091364244721356\n",
      "-2.2082504724051657\n",
      "-2.693900323261255\n",
      "-2.985852775683907\n",
      "-3.771113585230839\n",
      "-3.197760317870707\n",
      "-4.804606309123762\n",
      "-4.521265737199087\n",
      "-5.662586539950461\n",
      "-2.7712048033518633\n",
      "-8.34233153880272\n",
      "-4.8401128043829935\n",
      "-8.085892534631444\n",
      "-6.125730217260898\n",
      "-5.01461349239891\n",
      "-6.372076263029214\n",
      "-1.4436756285354386\n",
      "16.49275749734336\n",
      "-100.4303794774413\n",
      "183\n",
      "0.5060436934909864\n",
      "1.8649253210354868\n",
      "1.9713803770401466\n",
      "1.0326902691388011\n",
      "1.4225159144128565\n",
      "0.48348225319113225\n",
      "1.8330714826912686\n",
      "1.282520442523819\n",
      "2.255508603697587\n",
      "0.3399823497823206\n",
      "1.8746605346629326\n",
      "1.8792803034280041\n",
      "1.9229761155767857\n",
      "2.3224281868687697\n",
      "2.088671993713414\n",
      "0.08022253514928367\n",
      "-2.559346752087225\n",
      "-0.9759971705115607\n",
      "0.6247260126823334\n",
      "0.06431328128607676\n",
      "-2.373411444522063\n",
      "1.1934524709208312\n",
      "1.0172618336857442\n",
      "-0.48180744672798254\n",
      "-0.5037377396882107\n",
      "-1.8458418264199041\n",
      "-1.898986216501961\n",
      "-1.541765854340572\n",
      "-2.2130558572995778\n",
      "-0.570022703536489\n",
      "-3.118842440049451\n",
      "-0.47951396172009025\n",
      "-3.724052872702193\n",
      "-3.3172086227010547\n",
      "-3.6879422271711464\n",
      "-3.709284864896742\n",
      "-3.856530037722965\n",
      "-3.8778867044511545\n",
      "-3.704035313288059\n",
      "-3.752925289023595\n",
      "-4.062575642522527\n",
      "-3.7466364015634084\n",
      "-3.9645188755207457\n",
      "-3.87012862446688\n",
      "-4.232855442203889\n",
      "-4.499773654588179\n",
      "-4.201534417307295\n",
      "-4.584797272079358\n",
      "-4.27544826978761\n",
      "-2.1152438644395737\n",
      "-4.678766224491267\n",
      "-1.7276416838408761\n",
      "-3.662807576081878\n",
      "-4.7085822706613225\n",
      "-0.9789826734225653\n",
      "-4.870067870144433\n",
      "-5.025301048262912\n",
      "-4.5243119291133915\n",
      "-3.6669051124414564\n",
      "-3.3852024833839116\n",
      "-2.1548426304128636\n",
      "-3.8935572523150244\n",
      "-3.83961583207735\n",
      "-2.2085970367077583\n",
      "-2.929916173266995\n",
      "-4.1868131159125666\n",
      "-3.4312716843134896\n",
      "-3.5969732955252742\n",
      "-4.300690141442867\n",
      "-3.3110258206166465\n",
      "-3.1776321428270395\n",
      "-5.13392140006003\n",
      "-4.956249518019099\n",
      "-6.120415279229758\n",
      "-2.01179406633951\n",
      "-4.513246579208121\n",
      "-6.013794378109137\n",
      "-2.642259405710106\n",
      "-1.50126698179477\n",
      "-1.2547822999383516\n",
      "-3.038087665410703\n",
      "-1.265136795263702\n",
      "-1.112877857148219\n",
      "-0.9743122728402932\n",
      "-0.622410215143857\n",
      "-2.958552727560452\n",
      "-5.554109064541098\n",
      "-0.18856936878372388\n",
      "-1.5496332121672958\n",
      "-0.3716084547346964\n",
      "-5.595060406922132\n",
      "-0.033448542470550846\n",
      "-1.3877279978044932\n",
      "-0.012431179120171665\n",
      "-0.6496697001731832\n",
      "-0.7084186149612401\n",
      "0.0839624669556428\n",
      "0.03175539688522577\n",
      "-1.3768035480822438\n",
      "-6.027464859358165\n",
      "-2.039882066292123\n",
      "-1.1564657987453961\n",
      "-1.2878189452163225\n",
      "-7.366695494430323\n",
      "8.047656299417023\n",
      "-12.635960756026915\n",
      "-2.4119145284682784\n",
      "-31.682881802694304\n",
      "-31.254989752146148\n",
      "-100.56304039537906\n",
      "293\n",
      "Replay!\n",
      "(1, 4)\n",
      "[[-0.7424994  -0.45306835  0.7971572  -0.6140944 ]]\n",
      "()\n",
      "(1, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_345 to have 2 dimensions, but got array with shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-f6e29f2e6281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Replay!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-198-dafd2cb44463>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(memory, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#print(\"state_pred_reward_after\", state_pred_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_pred_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_pred_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-196-09842266113a>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, state, action, future_discounted_reward)\u001b[0m\n\u001b[1;32m     30\u001b[0m             model.fit(np.array(state), \n\u001b[1;32m     31\u001b[0m                       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_discounted_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                       epochs=1, verbose=0)\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#future_dsicounted_reward = future_discounted_reward[action]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_345 to have 2 dimensions, but got array with shape ()"
     ]
    }
   ],
   "source": [
    "model = Model(env, sc)\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    \n",
    "    EPSILON = 1.0 / np.sqrt(e+1)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    frames = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Perform action\n",
    "        \n",
    "        #env.render()\n",
    "        action = model.action(state, EPSILON)\n",
    "        prev_state = state\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Predict future discounted reward from current state and train model to find this mapping\n",
    "\n",
    "        pred_reward = model.predict(state)\n",
    "        future_discounted_reward = reward + GAMMA * np.max(pred_reward)\n",
    "        model._fit(prev_state, action, future_discounted_reward)\n",
    "        episode_reward += reward\n",
    "        frames += 1\n",
    "        \n",
    "        remember(prev_state, action, future_discounted_reward, state, done)\n",
    "        \n",
    "    episode_rewards[e] = episode_reward\n",
    "    \n",
    "    print(len(memory))\n",
    "    if len(memory) > 250: # 250\n",
    "        print(\"Replay!\")\n",
    "        replay(memory, BATCH_SIZE)\n",
    "    \n",
    "    if len(memory) >= 1000:\n",
    "        print(\"Clear!\")\n",
    "        memory.clear()\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print(\"Episode: \", e, \"Itr\", frames, \"Reward:\", episode_reward, \"Epsilon: %.3f\" % EPSILON, \"Avg reward (25):\", episode_rewards[max(0, e-25):(e+1)].mean())\n",
    "    if mission_accomplished(episode_rewards, e):\n",
    "        break\n",
    "\n",
    "print(\"Avg reward for last 100 episodes:\", episode_rewards[-100:].mean())\n",
    "print(\"Total steps:\", episode_rewards.sum())\n",
    "\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
